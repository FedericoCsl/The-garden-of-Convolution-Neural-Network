{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807d7380-bd86-4376-9b6b-f7a18c1e25f9",
   "metadata": {},
   "source": [
    "# The garden of Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b974ea54-3513-498c-bef5-2ebc95d2f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libreries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fbf57-6aa1-4aa3-837d-7cfe713fd641",
   "metadata": {},
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfc2dd-1d2f-4292-948b-06e90e3af50e",
   "metadata": {},
   "source": [
    "#### Training Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0f43d8b-f18f-403d-ac12-9ae01e72ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3680 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "datagenerator_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_set = datagenerator_train.flow_from_directory(\n",
    "                \"train_set\", target_size=(64,64),batch_size=32,class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f712853-6dbd-4a2d-9d77-6da5d9e8f910",
   "metadata": {},
   "source": [
    "#### Test Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ff45d53-12a6-4ce2-80a1-5434562bd01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 637 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "datagenerator_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = datagenerator_test.flow_from_directory(\n",
    "               \"test_set\", target_size=(64,64),batch_size=32,class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d74a7-4c5b-450d-9840-18ca77e55782",
   "metadata": {},
   "source": [
    "### How to build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ea58d75-037f-4cba-ac43-dfadf23e42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cca735-e13c-4e45-a737-e7d196ad9bd2",
   "metadata": {},
   "source": [
    "### How to build the Convolution Layer  --https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed262776-87ec-4c17-99ea-b9e5b7350683",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation=\"relu\",input_shape=[64,64,3])) #Convolution layer (CONV)\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))  # Pooling (POOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "868a5fa4-ab14-497d-b2b1-23ecf51d712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give at least 2 layers to make the model more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "772a7be7-bdf9-47cf-b193-c75b94c84ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation=\"relu\")) #Convolution layer (CONV)\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))  # Pooling (POOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c711ea0f-16d3-46a4-8736-1e5bb4577ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b5947bf-d5b8-446f-b2c4-4e78fde1ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8749872-7f26-4315-b202-b2ab0d4f1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation=\"relu\")) # for hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32569392-7327-4d7d-94d2-c360f5fa77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=5,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bde39a-c88d-47ee-afcf-be4533aa8cf5",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df98815-8f19-4cdf-8b97-d78012f2c2f3",
   "metadata": {},
   "source": [
    "###### if you check on the website \"https://keras.io/api/optimizers/\" you will find different types of optimizers, after trying a few ones, the one with the best result is RMSprop, lets check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca215f3-3f4e-4972-af7b-c6b624515100",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e42533b9-f5e3-487b-88ea-87e7eafa3f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "115/115 [==============================] - 124s 1s/step - loss: 1.3460 - accuracy: 0.4204 - val_loss: 1.0670 - val_accuracy: 0.5777\n",
      "Epoch 2/30\n",
      "115/115 [==============================] - 133s 1s/step - loss: 1.0760 - accuracy: 0.5674 - val_loss: 1.3837 - val_accuracy: 0.5055\n",
      "Epoch 3/30\n",
      "115/115 [==============================] - 134s 1s/step - loss: 0.9841 - accuracy: 0.6185 - val_loss: 1.0142 - val_accuracy: 0.6327\n",
      "Epoch 4/30\n",
      "115/115 [==============================] - 142s 1s/step - loss: 0.9043 - accuracy: 0.6495 - val_loss: 0.9377 - val_accuracy: 0.6311\n",
      "Epoch 5/30\n",
      "115/115 [==============================] - 128s 1s/step - loss: 0.8480 - accuracy: 0.6704 - val_loss: 1.0894 - val_accuracy: 0.6122\n",
      "Epoch 6/30\n",
      "115/115 [==============================] - 128s 1s/step - loss: 0.7889 - accuracy: 0.6965 - val_loss: 0.9047 - val_accuracy: 0.6829\n",
      "Epoch 7/30\n",
      "115/115 [==============================] - 114s 994ms/step - loss: 0.7747 - accuracy: 0.7008 - val_loss: 0.9364 - val_accuracy: 0.6578\n",
      "Epoch 8/30\n",
      "115/115 [==============================] - 118s 1s/step - loss: 0.7438 - accuracy: 0.7163 - val_loss: 0.7597 - val_accuracy: 0.6986\n",
      "Epoch 9/30\n",
      "115/115 [==============================] - 125s 1s/step - loss: 0.7100 - accuracy: 0.7321 - val_loss: 0.8322 - val_accuracy: 0.7159\n",
      "Epoch 10/30\n",
      "115/115 [==============================] - 115s 998ms/step - loss: 0.6727 - accuracy: 0.7503 - val_loss: 0.8326 - val_accuracy: 0.7111\n",
      "Epoch 11/30\n",
      "115/115 [==============================] - 127s 1s/step - loss: 0.6714 - accuracy: 0.7489 - val_loss: 0.8998 - val_accuracy: 0.6625\n",
      "Epoch 12/30\n",
      "115/115 [==============================] - 114s 992ms/step - loss: 0.6358 - accuracy: 0.7658 - val_loss: 0.8068 - val_accuracy: 0.7080\n",
      "Epoch 13/30\n",
      "115/115 [==============================] - 118s 1s/step - loss: 0.6191 - accuracy: 0.7750 - val_loss: 0.8107 - val_accuracy: 0.7206\n",
      "Epoch 14/30\n",
      "115/115 [==============================] - 111s 960ms/step - loss: 0.5975 - accuracy: 0.7690 - val_loss: 0.9643 - val_accuracy: 0.6766\n",
      "Epoch 15/30\n",
      "115/115 [==============================] - 129s 1s/step - loss: 0.5812 - accuracy: 0.7807 - val_loss: 0.9646 - val_accuracy: 0.6876\n",
      "Epoch 16/30\n",
      "115/115 [==============================] - 147s 1s/step - loss: 0.5691 - accuracy: 0.7861 - val_loss: 0.8238 - val_accuracy: 0.7190\n",
      "Epoch 17/30\n",
      "115/115 [==============================] - 186s 2s/step - loss: 0.5604 - accuracy: 0.7899 - val_loss: 0.8536 - val_accuracy: 0.7033\n",
      "Epoch 18/30\n",
      "115/115 [==============================] - 141s 1s/step - loss: 0.5429 - accuracy: 0.7995 - val_loss: 0.9148 - val_accuracy: 0.7111\n",
      "Epoch 19/30\n",
      "115/115 [==============================] - 109s 943ms/step - loss: 0.5172 - accuracy: 0.8005 - val_loss: 0.9424 - val_accuracy: 0.7017\n",
      "Epoch 20/30\n",
      "115/115 [==============================] - 130s 1s/step - loss: 0.5064 - accuracy: 0.8133 - val_loss: 0.9777 - val_accuracy: 0.6829\n",
      "Epoch 21/30\n",
      "115/115 [==============================] - 100s 864ms/step - loss: 0.4992 - accuracy: 0.8171 - val_loss: 0.9448 - val_accuracy: 0.6954\n",
      "Epoch 22/30\n",
      "115/115 [==============================] - 98s 853ms/step - loss: 0.4856 - accuracy: 0.8234 - val_loss: 0.8857 - val_accuracy: 0.7002\n",
      "Epoch 23/30\n",
      "115/115 [==============================] - 100s 865ms/step - loss: 0.4638 - accuracy: 0.8332 - val_loss: 0.8507 - val_accuracy: 0.7221\n",
      "Epoch 24/30\n",
      "115/115 [==============================] - 104s 903ms/step - loss: 0.4605 - accuracy: 0.8345 - val_loss: 0.9235 - val_accuracy: 0.7049\n",
      "Epoch 25/30\n",
      "115/115 [==============================] - 135s 1s/step - loss: 0.4689 - accuracy: 0.8372 - val_loss: 0.9466 - val_accuracy: 0.6813\n",
      "Epoch 26/30\n",
      "115/115 [==============================] - 119s 1s/step - loss: 0.4340 - accuracy: 0.8405 - val_loss: 1.0065 - val_accuracy: 0.6954\n",
      "Epoch 27/30\n",
      "115/115 [==============================] - 107s 933ms/step - loss: 0.4174 - accuracy: 0.8492 - val_loss: 1.0536 - val_accuracy: 0.6860\n",
      "Epoch 28/30\n",
      "115/115 [==============================] - 134s 1s/step - loss: 0.4271 - accuracy: 0.8484 - val_loss: 1.0619 - val_accuracy: 0.6656\n",
      "Epoch 29/30\n",
      "115/115 [==============================] - 109s 943ms/step - loss: 0.4147 - accuracy: 0.8438 - val_loss: 1.4087 - val_accuracy: 0.6421\n",
      "Epoch 30/30\n",
      "115/115 [==============================] - 135s 1s/step - loss: 0.3942 - accuracy: 0.8527 - val_loss: 1.0363 - val_accuracy: 0.6970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2894f880400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_set,validation_data=test_set,epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03294e-9e55-4fa9-9bff-d81ff3f440bd",
   "metadata": {},
   "source": [
    "#### Now its time to preprocess the pics I took from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04145918-fd8e-4c72-81e9-a2ff5de1d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(\"prediction/daisy_pic.jpg\",target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77731a7c-de7b-4473-8127-ec59104a65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35818454-95ef-4d25-9310-2bca2d7e51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a4657-cfc5-4c76-b30a-35bbfb07bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result[0][0]==1:\n",
    "    print(\"Daisy\")\n",
    "elif result[0][1]==1:\n",
    "    print(\"Dandelion\")\n",
    "elif result[0][2]==1:\n",
    "    print(\"Rose\")\n",
    "elif resul[0][3]==1:\n",
    "    print(\"Sunflower\")\n",
    "elif result[0][4]==1:\n",
    "    print(\"Tulip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb3c66-1a63-4ae4-8316-78a2ead56813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
